{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Analyzing Airline Flight Delays\n",
    "For a full treatment of the unit 14 case study, please review module 14.3. Some points from the video are given below.\n",
    "\n",
    "Work with the airline data set (use R or Python to manage out-of-core).\n",
    "Answer the following questions by using the split-apply-combine technique:\n",
    "* Which airports are most likely to be delayed flying out of or into?\n",
    "* Which flights with same origin and destination are most likely to be delayed?\n",
    "* Can you regress how delayed a flight will be before it is delayed?\n",
    "* What are the most important features for this regression?\n",
    "\n",
    "Remember to properly cross-validate models.\n",
    "\n",
    "Use meaningful evaluation criteria.\n",
    "\n",
    "Create at least one new feature variable for the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd #http://dask.pydata.org/en/latest/\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "### Other Settings\n",
    "# Show more rows\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "# Prevent scientific notation of decimals\n",
    "pd.set_option('precision',3)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# Data Location\n",
    "# Ryan's\n",
    "parq_folder = \"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9c7d159d-ac49-4fe1-a739-14a7bf09f4a0\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"9c7d159d-ac49-4fe1-a739-14a7bf09f4a0\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9c7d159d-ac49-4fe1-a739-14a7bf09f4a0\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9c7d159d-ac49-4fe1-a739-14a7bf09f4a0' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"9c7d159d-ac49-4fe1-a739-14a7bf09f4a0\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"9c7d159d-ac49-4fe1-a739-14a7bf09f4a0\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Allow inline display of bokeh graphics\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Here is some info about Dask]...\n",
    "\n",
    "...General facts about Dask... blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Dask Files\n",
    "* Ryan's Hardware: \n",
    "    - CPU: Intel i5-4300M @ 2.60GHz\n",
    "    - Disk: Samsung SSD 850 Pro\n",
    "    - RAM: 8 GB\n",
    "    \n",
    "\n",
    "* Dask using original csv:\n",
    "    - no conversion\n",
    "    - size on disk\n",
    "        - 11.2 gb\n",
    "    - benchmark of describing 'Distance':\n",
    "        - Approx. 4 minutes\n",
    "* Dask using uncompressed parquet: \n",
    "    - conversion to parquet\n",
    "        - approx 10 minutes\n",
    "    - size on disk:\n",
    "        - 13.8 gb\n",
    "    - benchmark of describing 'Distance':\n",
    "        - 1 loop, best of 3: 6.2 s per loop\n",
    "* Dask using gzip compressed parquet:\n",
    "    - converstion to parquet\n",
    "        - approx 42 minutes\n",
    "    - size on disk:\n",
    "        - 1.36 gb <- big difference\n",
    "    - benchmark of describing 'Distance':\n",
    "        - 1 loop, best of 3: 8.83 s per loop\n",
    "\n",
    "#### Summary\n",
    "Dask allows for out of core management of data sets. CSV files are universal, but slow to process. Converting to parquet file format, speeds up the process by a factor of 38. Using the gzip compression, reduces size on disk from 13.8gb to 1.36 or about 10% of the uncompressed size. This comes in handy for a distributed processing in a cluster since not as much network bandwidth would be needed. The trade off of compression is a 42.4% increasing in processing time, however, 3 additional seconds is hardly noticable, but might be more of an issue for other tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data types of the fields used in the analysis\n",
    "dts = {'ActualElapsedTime': 'float64', # Confirmed\n",
    " 'AirTime': 'float64', # Confirmed\n",
    " 'ArrDelay': 'float64', # Confirmed\n",
    " 'ArrTime': 'float64', # Confirmed\n",
    " 'CRSArrTime': 'int64', # Confirmed\n",
    " 'CRSDepTime': 'int64', # Confirmed\n",
    " 'CRSElapsedTime': 'float64', # Confirmed\n",
    " 'CancellationCode': 'O', # Confirmed by lesson video\n",
    " 'Cancelled': 'int64', # Confirmed\n",
    " 'CarrierDelay': 'float64', # Confirmed\n",
    " 'DayOfWeek': 'int64', # Confirmed\n",
    " 'DayofMonth': 'int64', # Confirmed\n",
    " 'DepDelay': 'float64', # Confirmed\n",
    " 'DepTime': 'float64', # Confirmed\n",
    " 'Dest': 'O', # Confirmed\n",
    " 'Distance': 'float64', # Confirmed\n",
    " 'Diverted': 'int64', # Confirmed\n",
    " 'FlightNum': 'int64', # Exploring if int or string\n",
    " 'LateAircraftDelay': 'float64', # Confirmed\n",
    " 'Month': 'int64', # Confirmed\n",
    " 'NASDelay': 'float64', # Confirmed\n",
    " 'Origin': 'O', # Confirmed\n",
    " 'SecurityDelay': 'float64', # Confirmed\n",
    " 'TailNum': 'O', # Confirmed\n",
    " 'TaxiIn': 'float64', # Confirmed\n",
    " 'TaxiOut': 'float64', # Confirmed\n",
    " 'UniqueCarrier': 'O', # Confirmed\n",
    " 'WeatherDelay': 'float64', # Confirmed\n",
    " 'Year': 'int64'} # Confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parquet time:  0:00:01.217000\n",
      "\n",
      "There are 123,534,969 rows\n",
      "Time to determine row count:  0:02:52.357000\n"
     ]
    }
   ],
   "source": [
    "# Load compressed Parquet format of all years\n",
    "start = datetime.now()\n",
    "all_years = dd.read_parquet(parq_folder)\n",
    "print(\"Load parquet time: \", datetime.now() - start)\n",
    "print()\n",
    "\n",
    "# Length of dask dataframe\n",
    "start = datetime.now()\n",
    "print(\"There are {:,d} rows\".format(len(all_years))) #123,534,969 Matches Eric Larson\n",
    "print(\"Time to determine row count: \", datetime.now() - start)\n",
    "\n",
    "# Tiny sample of \n",
    "all_years.sample(.00001).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to determine row count:  0:00:40.050300 \n",
      "\n",
      "            Count\n",
      "Origin           \n",
      "ORD     6,597,442\n",
      "ATL     6,100,953\n",
      "DFW     5,710,980\n",
      "LAX     4,089,012\n",
      "PHX     3,491,077\n",
      "DEN     3,319,905\n",
      "DTW     2,979,158\n",
      "IAH     2,884,518\n",
      "MSP     2,754,997\n",
      "SFO     2,733,910\n"
     ]
    }
   ],
   "source": [
    "# Busiest Origins\n",
    "start = datetime.now()\n",
    "origin_counts = (all_years[['Origin','Year']].groupby('Origin').count().compute()\n",
    "                 .sort_values(by='Year', ascending=False)\n",
    "                 .rename(columns={\"Year\":\"Count\"})\n",
    "                )\n",
    "\n",
    "print(\"Time to determine row count: \", datetime.now() - start, \"\\n\")\n",
    "format = lambda x: \"{0:,.0f}\".format(x) \n",
    "print(origin_counts[:10].applymap(format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Which airports are most likely to be delayed flying out of or into?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Which flights with same origin and destination are most likely to be delayed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you regress how delayed a flight will be before it is delayed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## What are the most important features for this regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Regression of Delay\n",
    "\n",
    "The Dask module is a solution for processing \"big data,\" however, the it currently does not include methods for analysis, such as generalized linear models, like other big data solutions. The following will use a series of simple random sampling and kfold cross validation to find the coefficient estimates of a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12075, 3)\n",
      "Coefficients: \n",
      " [[ 0.00183383  0.01027518]]\n",
      "Time to sample and regress:  0:00:19.308600\n",
      "(12075, 3)\n",
      "Coefficients: \n",
      " [[  1.98780496e-05   1.05399143e-02]]\n",
      "Time to sample and regress:  0:00:36.565000\n",
      "(12075, 3)\n",
      "Coefficients: \n",
      " [[ 0.00089394  0.00993085]]\n",
      "Time to sample and regress:  0:00:53.828000\n",
      "(12075, 3)\n",
      "Coefficients: \n",
      " [[ 0.00061025  0.01016242]]\n",
      "Time to sample and regress:  0:01:13.605000\n",
      "(12075, 3)\n",
      "Coefficients: \n",
      " [[ 0.0001067   0.01025777]]\n",
      "Time to sample and regress:  0:01:36.391100\n",
      "[array([[ 0.00183383,  0.01027518]]), array([[  1.98780496e-05,   1.05399143e-02]]), array([[ 0.00089394,  0.00993085]]), array([[ 0.00061025,  0.01016242]]), array([[ 0.0001067 ,  0.01025777]])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "seeds = [123,456,789,101,112]\n",
    "\n",
    "coefs = []\n",
    "\n",
    "\n",
    "# Sample the entire data set as large as possible a few times. Each time has it's own cross validation sampling.\n",
    "start = datetime.now()\n",
    "for i in range(len(seeds)):\n",
    "    # Take a sample from all the data\n",
    "    all_years_reg = all_years[['ArrDelay','Distance', 'DepTime']].dropna().sample(.0001, random_state=seeds[i]).compute()\n",
    "    #print(all_years_reg.info())\n",
    "    print(all_years_reg.shape)\n",
    "        \n",
    "    ######\n",
    "    # Insert a cross validation split step here\n",
    "    ######\n",
    "    \n",
    "    reg = linear_model.LinearRegression()\n",
    "    ArrDelay_X = all_years_reg[['Distance', 'DepTime']]\n",
    "    ArrDelay_y = all_years_reg[['ArrDelay']]\n",
    "    reg.fit(ArrDelay_X, ArrDelay_y)\n",
    "    print('Coefficients: \\n', reg.coef_)\n",
    "    coefs.append(reg.coef_)\n",
    "    print(\"Time to sample and regress: \", datetime.now() - start)\n",
    "\n",
    "print(coefs) # Chart this eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00045107047014612947"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "* Optimize with index key base on Data, deptarture time, and TailNum\n",
    "* Use of alternative compression, such as snappy or LZ4\n",
    "    * http://java-performance.info/performance-general-compression/\n",
    "* Use a diffent big data approach to find a more efficient way to estimating the linear model coefficients:\n",
    "    * Spark MLLib\n",
    "    * Dask GLM\n",
    "    * Turi/Graphlab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* Dask Documentation, http://dask.pydata.org/en/latest/\n",
    "* Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, Boyd, et al http://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A - CSV to Parquet Conversion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Convert csv to parquet\n",
    "csv_folder = \"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/*.csv\"\n",
    "parq_folder = \"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/parquet/\"\n",
    "\n",
    "def csv_to_parquet(csv_folder, parq_folder):\n",
    "    start = datetime.now()\n",
    "    df_csv = dd.read_csv(csv_folder,                       \n",
    "                         usecols = dts.keys(),\n",
    "                         dtype=dts, \n",
    "                         encoding='iso-8859-1')\n",
    "    \n",
    "    # Flip to parquet\n",
    "    df_csv.to_parquet(parq_folder,\n",
    "                      compression='gzip',\n",
    "                      object_encoding='utf8')\n",
    "\n",
    "    time_to_complete = datetime.now() - start\n",
    "    print(time_to_complete)\n",
    "\n",
    "csv_to_parquet(csv_folder, parq_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B - Benchmark Tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def benchmark_test(df):\n",
    "    format = lambda x: \"{0:.3f}\".format(x) \n",
    "    start = datetime.now()\n",
    "    print(df[['Distance']].dropna().describe().compute().applymap(format))\n",
    "    time_to_complete = datetime.now() - start\n",
    "    print(time_to_complete)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_par = dd.read_parquet(\"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/parquet/\")\n",
    "%timeit benchmark_test(df_par)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#%timeit Using this magic causes the memory error\n",
    "df_csv = dd.read_csv(\"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/*.csv\", \n",
    "                 usecols = dts.keys(),\n",
    "                 dtype=dts, \n",
    "                 encoding='iso-8859-1')\n",
    "\n",
    "start = datetime.now()\n",
    "benchmark_test(df_csv)\n",
    "print(datetime.now() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
