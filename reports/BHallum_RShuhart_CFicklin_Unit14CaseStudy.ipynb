{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Analyzing Airline Flight Delays \n",
    "#### By Brett Hallum, Chris Ficklin, and Ryan Shuhart<br>April 2017\n",
    "\n",
    "We worked with the airline data set with a goal to use Python and its libraries to manage this large data set with out-of-core memory.\n",
    "\n",
    "We wanted answers the following questions with use of the split-apply-combine technique:\n",
    "* Which airports are most likely to be delayed flying out of or into?\n",
    "* Which flights with same origin and destination are most likely to be delayed?\n",
    "* Can you regress how delayed a flight will be before it is delayed?\n",
    "* What are the most important features for this regression?\n",
    "\n",
    "Within the scope of answering these questions, the models were cross-validated using sampling techniques and evalutated using criteria and standards set up through the FAA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd #http://dask.pydata.org/en/latest/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "total_time = datetime.now()\n",
    "\n",
    "# from dask.distributed import Client\n",
    "# client = Client(set_as_default=True)\n",
    "# print(client)\n",
    "\n",
    "### Other Settings\n",
    "# Show more rows\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "# Prevent scientific notation of decimals\n",
    "pd.set_option('precision',3)\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Allow inline display of bokeh graphics\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "Dask is a partial implementation of Pandas to split a large dataframe into many smaller Pandas dataframes. This library uses parallel computation for its analytics and is composed of a task scheduler and Big Data collections, such as the dataframe we will use. Dask is fast, flexible and scalable for many clusters with 1000â€™s of cores. Dask is easy to use because it mimics a lot of the API syntax of pandas and numpy to calculate various values. The data frames and groupby functions are nearly identical between dask and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flight data contained 29 variables with various data types. In order to use Dask in an appropriate manner, these data types all needed to be converted to numeric values. This is done further down below after reading in the data from the parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stat-computing.org/dataexpo/2009/the-data.html\n",
    "var_desc = pd.read_csv(\"../ref/var_descriptions.csv\", index_col='var_id')\n",
    "var_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Location\n",
    "#parq_folder = \"../data/parquet-tiny/\" # Testing\n",
    "#parq_folder = \"../data/parquet_25/\" # Higher Load Testing\n",
    "parq_folder = \"../data/parquet/\" # Full data\n",
    "\n",
    "# Load compressed Parquet format of all years ~2 sec\n",
    "start = datetime.now()\n",
    "# All data\n",
    "df = dd.read_parquet(parq_folder)\n",
    "\n",
    "# Length of dask dataframe ~3 min\n",
    "start = datetime.now()\n",
    "print(\"There are {:,d} observations in all the data.\".format(len(df))) #123,534,969 Matches Eric Larson\n",
    "# print(\"There are {:,d} observations after 1994.\".format(len(df_with_tails))) \n",
    "print(\"Time to determine row counts: \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data set was pulled in a CSV format. We wanted to convert to the parquet format in order to better store the data and analyze it within Dask. Parquet is a binary data store format that is columnar storage. It allows data to be split and run in parallel. This conversion can be seen in Appendix A.\n",
    "\n",
    "We load all of the data into a single dask dataframe for use within the rest of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glance at Beginning and End of Dask Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Last 5 rows:\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preparation and Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparing the files for analysis, we also wanted to create new features that could be used throughout the rest of the processing. There are 2 features created for the regression with 2 additional features as intermediaries. The first feature is the Hour variable. As visualized in a following section, later hours have more delays. It is transformed from the scheduled departure time (CRSDepTime) to the hour of the day. \n",
    "\n",
    "The other feature created is the estimated plane age. The age could be a potential factor in delays and will be used during the regression analysis of the data. The age of a plane is estimated from the day of the first flight found in the data. It is calculated by determining the number of months from 0 A.D. for each flight, and the first flight of each plane. The difference results in the number of months between the first flight and each flight, or the estimated plane age in months. \n",
    "\n",
    "In addition, Hour, Distance, and Age are standardized to a z-score as new fields in order to determine the comparable feature importance.  \n",
    "\n",
    "For faster processing of the regression, a processed data set is created after the features and z-score scaling. The code for the features and scaling can be located in Appendix D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an hour field\n",
    "# 2400 minutes from midnight reduced to 2399 then int division drops to 23\n",
    "df = df.assign(Hour=df.CRSDepTime.astype(float).clip(upper=2399)//100) \n",
    "\n",
    "# Months from 0 AD\n",
    "df['FlightAge'] = 12*df['Year']+df['Month']-1\n",
    "\n",
    "# The months from the first recorded flight is consider the approx age of the plane. \n",
    "# Unfortunately, tail numbers not tracked until 1995. \n",
    "\n",
    "# Find the first year and month of a tail numbers flight history\n",
    "tail_births = (df.groupby('TailNum')[['FlightAge']].min().reset_index()\n",
    "                 .rename(columns={'FlightAge':'FirstFlight'}))\n",
    "\n",
    "df_with_tails = dd.merge(df[df['Year']>1994], tail_births, how='left', on='TailNum')\n",
    "df_with_tails['Age'] = df_with_tails['FlightAge'] - df_with_tails['FirstFlight']\n",
    "\n",
    "#df_with_tails = df_with_tails.drop(['FlightAge','FirstFlight'], axis=1)\n",
    "\n",
    "start = datetime.now()\n",
    "def scaler(df, column):\n",
    "    return (df[column] - df[column].dropna().mean())/df[column].dropna().std()\n",
    "\n",
    "# Scale columns for regression of all data\n",
    "df['Hour_scaled'] = scaler(df, 'Hour')\n",
    "df['Distance_scaled'] = scaler(df, 'Distance')\n",
    "\n",
    "# Scale columns for regression for after 1994\n",
    "df_with_tails['Hour_scaled'] = scaler(df_with_tails, 'Hour')\n",
    "df_with_tails['Distance_scaled'] = scaler(df_with_tails, 'Distance')\n",
    "df_with_tails['Age_scaled'] = scaler(df_with_tails, 'Age')\n",
    "\n",
    "print(\"Time to Build: \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Delays\n",
    "\n",
    "According to the FAA, when a schedule airflight departs more than 15 minutes after its scheduled time, it is considered officially delayed. We utilize the same logic for arrival times to determine if a flight is arriving late and is therefore delayed by arrival rather than by departure. Only departures and arrivals 15 minutes past the scheduled time will be considered late in the analysis.\n",
    "\n",
    "http://aspmhelp.faa.gov/index.php/Types_of_Delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "When running Dask, you can view visualizations of the distrubuted work by going to the host and port below. This uses the bokehJS library which watches the computer's cores and gives visual feedback of what is happening on all of the cores while also tracking the list of completed and non-completed tasks for the given block of code.\n",
    "\n",
    "Below we note the conversion of several of the variables to numeric values with the use of categorization. This helps make the data set fully numeric so Dask runs in the correct manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "start = datetime.now()\n",
    "\n",
    "# Make Categories as categorical. Making categorical reduces run time from ~10min to ~3min \n",
    "# with the process of making the variables categorical\n",
    "df = df.categorize(['DayOfWeek', 'UniqueCarrier', 'Dest', 'Origin'])\n",
    "\n",
    "# Define some aggregations to plot\n",
    "aggregations = (\n",
    "    #1 Average departure delay by year\n",
    "    df.groupby('Year').DepDelay.mean(),\n",
    "    \n",
    "    #2 Average departure delay by Month\n",
    "    df.groupby('Month').DepDelay.mean(), \n",
    "    \n",
    "    #3 Average departure delay by hour of day\n",
    "    df.groupby('Hour').DepDelay.mean(), \n",
    "    \n",
    "    #4 Average departure delay by Carrier, top 15\n",
    "    df.groupby('UniqueCarrier').DepDelay.mean().nlargest(15), \n",
    "    \n",
    "    #5 Average arrival delay by destination, top 15\n",
    "    (df.groupby('Dest').ArrDelay.mean().nlargest(15) \n",
    "     .reset_index().rename(columns={'ArrDelay':'AvgArrDelay'})),\n",
    "    \n",
    "    #6 Count of arrivals to destinations, excludes missing\n",
    "    (df.groupby('Dest').ArrDelay.count() \n",
    "     .reset_index().rename(columns={'ArrDelay':'ArrCount'})),\n",
    "    \n",
    "    #7 Average departure delay by origin, top 15\n",
    "    (df.groupby('Origin').DepDelay.mean().nlargest(15).reset_index().rename(columns={'DepDelay':'AvgDepDelay'})),\n",
    "    \n",
    "    #8 Count of departures by origin, excludes missing\n",
    "    (df.groupby('Origin').DepDelay.count().reset_index().rename(columns={'DepDelay':'DepCount'})), \n",
    "    \n",
    "    #9 Average departure by origin and destination\n",
    "    (df.groupby(['Origin','Dest']).DepDelay.mean().reset_index().rename(columns={'DepDelay':'AvgDepDelay'})),\n",
    "    \n",
    "    #10 Count of departures between origin and destination\n",
    "    (df.groupby(['Origin','Dest']).DepDelay.count().reset_index().rename(columns={'DepDelay':'DepCount'})),\n",
    "    \n",
    "    #11 Percentage of officially delayed flights by origin\n",
    "    ((df[df.DepDelay>15].groupby('Origin').DepDelay.count() / df.groupby('Origin').DepDelay.count())\n",
    "     .reset_index().rename(columns={'DepDelay':'PercDepDelay'})),\n",
    "    \n",
    "    #12 Percentage of officially late flights by destination\n",
    "    ((df[df.ArrDelay>15].groupby('Dest').ArrDelay.count() / df.groupby('Dest').ArrDelay.count())\n",
    "     .reset_index().rename(columns={'ArrDelay':'PercArrDelay'})),\n",
    "                \n",
    "    #13 Percentage of officially delayed flights by origin and destination\n",
    "    ((df[df.DepDelay>15].groupby(['Origin','Dest']).DepDelay.count() / df.groupby(['Origin','Dest']).DepDelay.count())\n",
    "     .reset_index().rename(columns={'DepDelay':'PercDepDelay'})),\n",
    "                \n",
    "    #14 Percentage of officially late flights by origin and destination\n",
    "    ((df[df.ArrDelay>15].groupby(['Origin','Dest']).ArrDelay.count() / df.groupby(['Origin','Dest']).ArrDelay.count())\n",
    "     .reset_index().rename(columns={'ArrDelay':'PercArrDelay'})),\n",
    "    \n",
    "    #15 Average departure delay by hour of day\n",
    "    df.groupby('DayOfWeek').DepDelay.mean()\n",
    ")\n",
    "from dask.diagnostics import Profiler\n",
    "\n",
    "with Profiler() as prof:\n",
    "    # Compute them all in a single command\n",
    "    (\n",
    "    delayed_by_year, #1\n",
    "    delayed_by_month, #2\n",
    "    delayed_by_hour, #3\n",
    "    delayed_by_carrier, #4\n",
    "    delayed_by_dest, #5\n",
    "    delayed_by_dest_count, #6\n",
    "    delayed_by_origin, #7\n",
    "    delayed_by_origin_count, #8\n",
    "    delayed_by_origin_dest, #9\n",
    "    delayed_by_origin_dest_count, #10\n",
    "    pct_delayed_by_origin, #11\n",
    "    pct_late_by_dest, #12\n",
    "    pct_delayed_by_origin_dest, #13\n",
    "    pct_late_by_origin_dest, #14\n",
    "    delayed_by_day #15\n",
    "    ) = dask.compute(*aggregations)\n",
    "\n",
    "prof.visualize()\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Average Delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We first take a look at the average delay for flights from the data set. Below, we look at the different representations for the average delay time for year, month, day, hour, and finally by carrier.\n",
    "\n",
    "For the average delay by year, there dont seem to be many deviations. There are two significant years higher than 10 minutes in 2000 and 2007. We aloso see several years with low average delays less than 6 minutes including 1991, 1992, 2002, and 2003.\n",
    "\n",
    "When we look at the average delay by month, we see December having the largest delay times with an average delay time of nearly 12 minutes. The month of September is the lowest month of delays. These highs and lows are most likely attributed to the flight patterns of people. Many people do not travel in September because school is starting back up and people have already taken time off for the summer. There are also a lot more travel conducted during December because of Christmas, one of the more popular holidays for travel.\n",
    "\n",
    "An interesting plot to observe is the average delay by hour. It seems that there is a constant increase in delay times as the day gets going. From 6am to around 8pm, there is a constant increase in the average delay time. It starts less than 2 minutes on average for delays and increases all the way up to nearly 14 minutes. After 8pm, the time decreases again into the early morning, leveling back to \"low\" levels less than 4 minutes around 2am.\n",
    "\n",
    "The average delay by day of the week does not have much to note. Delays are fairly stable around 8 minutes on average for each day, with day 5, Friday, being higher at 10 minutes. This is most likely due to high travel frequency on that day.\n",
    "\n",
    "Finally, we see the average delay be carrier. A few of the first carriers in the plot have average delay times larger than 12 minutes. This decreases, but none of the carriers seem to fall below an average delay time of 8 minutes. This is most likely due to the fact that nearly every carrier has to deal with similar factors such as weather, airport issues, and leveraging the control tower for take off. No single carrier should have vastly different times than the others if all things are equal for every carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.charts.attributes import cat\n",
    "from bokeh.charts import Bar\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "# Average Delay by Year\n",
    "p1 = Bar(delayed_by_year.reset_index(), 'Year', values= 'DepDelay', \n",
    "         legend=False, ylabel=\"Average Delay in Minutes\", \n",
    "         title=\"Average Delay by Year\")\n",
    "\n",
    "# Average Delay by Month\n",
    "delayed_by_month = delayed_by_month.sort_index()\n",
    "p2 = Bar(delayed_by_month.reset_index(), 'Month', values= 'DepDelay', \n",
    "         legend=False, ylabel=\"Average Delay in Minutes\", \n",
    "         title=\"Average Delay by Month\")\n",
    "\n",
    "# Average Delay by Hour of Day\n",
    "p3 = Bar(delayed_by_hour.reset_index(), 'Hour', values= 'DepDelay', \n",
    "         legend=False, ylabel=\"Average Delay in Minutes\",\n",
    "         title=\"Average Delay by Hour of Day\")\n",
    "\n",
    "# Average Delay by Hour of Day\n",
    "p4 = Bar(delayed_by_day.reset_index(), 'DayOfWeek', values= 'DepDelay', \n",
    "         legend=False, ylabel=\"Average Delay in Minutes\",\n",
    "         title=\"Average Delay by Day of Week\")\n",
    "\n",
    "# Average Delay by Carrier\n",
    "delayed_by_carrier = delayed_by_carrier.reset_index()\n",
    "delayed_by_carrier['UniqueCarrier'] = delayed_by_carrier['UniqueCarrier'].astype('O')\n",
    "p5 = Bar(delayed_by_carrier, label=cat('UniqueCarrier', sort=False), values= 'DepDelay', \n",
    "         legend=False, ylabel=\"Average Delay in Minutes\", xlabel=\"Unique Carrier\", title=\"Average Delay by Carrier\")\n",
    "\n",
    "\n",
    "show(gridplot([[p1,p2],[p3,p4], [p5,None]], plot_width=400, plot_height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which airports are most likely to be delayed flying out of or into?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis conducted below looks at the combination of departure delays and arrival delays for each airport in the dataset. The average of these two values is what is used to determine which airport is the most likely to have delays.\n",
    "\n",
    "From our analysis, we see that there are 7 airports that have an average delay in over 30% of their flights. This includes the largest delay we see at Moore County Airport (SOP) which averages delays 39.1% of the time. This is due to departure delays that occur 36.8% of the time and arrival delays that occur 41.4% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airport_delays_pcts = (pd.merge(pct_delayed_by_origin, pct_late_by_dest, left_on='Origin', right_on='Dest')\n",
    "                         .assign(AvgDelay= lambda x: (x['PercDepDelay'] + x['PercArrDelay'])/2)\n",
    "                         .sort_values(by='AvgDelay', ascending=False)\n",
    "                         .drop('Dest', axis=1)\n",
    "                )\n",
    "\n",
    "airport_delays_pcts = pd.merge(airport_delays_pcts, delayed_by_origin_count, on='Origin')\n",
    "\n",
    "airport_delays_pcts[airport_delays_pcts['DepCount'] > 50].nlargest(15, 'AvgDelay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which flights with same origin and destination are most likely to be delayed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question we want to answer are which typical flights are more likely to be delayed. Using a similar process as conducted previously, we look at the departure delays and arrival delays and average the value to get the total value of delayed flights. For this question, however, we want all flights where the origin and destination are the same instead of looking at individual airports.\n",
    "\n",
    "The worst flight seems to be the flights that fly from Jackson International (JAN) to Baton Rouge Metropolitan (BTR). This flight is delayed 64.6% of the time on average. The delays occur 58.2% of the time for departures and a significant 70.9% of the time for arrivals.\n",
    "\n",
    "These result tells us that if you are taking this flight, or any flights that are high on this list, then you can most likely expect a delay, either leaving from the origin or arriving to the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "org_dest_pcts = (pd.merge(pct_delayed_by_origin_dest, pct_late_by_origin_dest, on=['Origin','Dest'])\n",
    "                 .assign(AvgDelay= lambda x: (x['PercDepDelay'] + x['PercArrDelay'])/2)\n",
    "                 .sort_values(by='AvgDelay', ascending=False)\n",
    "                )\n",
    "\n",
    "org_dest_pcts = pd.merge(org_dest_pcts, delayed_by_origin_dest_count, on=['Origin','Dest'])\n",
    "\n",
    "org_dest_pcts[org_dest_pcts['DepCount'] > 50].nlargest(15, 'AvgDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import Histogram\n",
    "\n",
    "hist = Histogram(df[df['DepDelay']>15][['DepDelay']].sample(.25).compute().dropna(), \n",
    "                 values='DepDelay', bins=50)\n",
    "\n",
    "show(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above shows the counts of different delay times for a sample of 25% the flights. Many of the flights fall within the first 2 bins of delays less than 100 minutes, with 3.5 million being less than 50 minutes and aroudn 750,000 being between 50 and 100 minutes. The rest fall in the range of 100 to around 500. There may be further values higher than this, but there are so few flights delayed longer than 500 minutes (over 8 hours) that they do not show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you regress how delayed a flight will be before it is delayed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most important features for this regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression of Delay\n",
    "\n",
    "The Dask module is a solution for processing \"big data,\" however, the it currently does not include built in methods for regression or classification, like other big data solutions. The following will use a series of simple random sampling to a size that fits into a pandas dataframe to find the coefficient estimates of a linear model. The coefficients will be averaged to make a final prediction. This process also assists in not over fitting the model.\n",
    "\n",
    "#### The following features will be explore to predict if the flight will have departure delay\n",
    "\n",
    "##### The predicted variable will be: \n",
    "* Departure Delay (DepDelay)\n",
    "\n",
    "##### The explanatory variables:\n",
    "* Scheduled departure hour (Hour)\n",
    "* Flight distance (Distance)\n",
    "* Age of plane (Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample the entire data set as large as possible a few times. Each time has it's own cross validation sampling.\n",
    "def sample_coef(Xcols, ycol, df, samp_size = .1, seeds = [123,456,789,101,112]):\n",
    "    t = datetime.now()\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import r2_score\n",
    "    import dask\n",
    "    reg = linear_model.LinearRegression(n_jobs=-1)\n",
    "    coefs = []\n",
    "    \n",
    "    for i in range(len(seeds)):\n",
    "        start = datetime.now()\n",
    "        # Take a sample from all the data\n",
    "        all_cols = [ycol] + Xcols\n",
    "        Xy = df[all_cols].sample(samp_size, random_state=seeds[i]).compute().dropna(axis=0)\n",
    "        X = Xy[Xcols]\n",
    "        y = Xy[ycol].values\n",
    "\n",
    "        reg.fit(X, y)\n",
    "        #print('Coefficients: \\n', reg.coef_)\n",
    "        coefs.append(reg.coef_)\n",
    "        print(\"Time for Sample {}: \".format((i+1)), datetime.now() - start)\n",
    "        #print(datetime.now() - start)\n",
    "    \n",
    "    del Xy, X, y\n",
    "\n",
    "    coef_df = pd.DataFrame.from_records(coefs, columns=Xcols)\n",
    "    coef_avg = coef_df.mean()\n",
    "    print(\"\\nCoefficients:\")\n",
    "    print(coef_df)    \n",
    "    print(\"\\nAverage Coefficients:\")\n",
    "    print(coef_avg)\n",
    "    \n",
    "    beta_cols = []\n",
    "    for m, c in zip(coef_avg.index, coef_avg.values):\n",
    "        b_col = \"Beta_\"+m\n",
    "        df[\"Beta_\"+m] = df[m]*c\n",
    "        beta_cols.append(b_col)\n",
    "\n",
    "    df['Predicted'] = df[beta_cols].sum(axis=1)\n",
    "\n",
    "    #df['SqError'] = (df['Predicted'] - df[ycol])**2\n",
    "    #mse = df[['SqError']].mean().compute()\n",
    "    \n",
    "    df_tmp = df[['Predicted']+[ycol]].sample(.4).compute().dropna()\n",
    "    y_true = df_tmp[ycol]\n",
    "    y_pred = df_tmp['Predicted']\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(\"\\nEstimated Evaluation Scores on 40%:\")\n",
    "    print(\"Mean Squared Error: \", mse)\n",
    "    print(\"R Squared: \", r2)\n",
    "    print(\"\\nTotal Time: \", datetime.now() - t)\n",
    "    return coef_df, coef_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xcols = ['Hour', 'Distance']\n",
    "ycol =  'DepDelay'\n",
    "coef_df1, coef_avg1 = sample_coef(Xcols, ycol, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xcols = ['Hour_scaled', 'Distance_scaled']\n",
    "ycol =  'DepDelay'\n",
    "coef_df2, coef_avg2 = sample_coef(Xcols, ycol, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xcols = ['Hour', 'Distance', 'Age']\n",
    "ycol =  'DepDelay'\n",
    "coef_df3, coef_avg3 = sample_coef(Xcols, ycol, df_with_tails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xcols = ['Hour_scaled', 'Distance_scaled', 'Age_scaled']\n",
    "ycol =  'DepDelay'\n",
    "coef_df4, coef_avg4 = sample_coef(Xcols, ycol, df_with_tails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Total Run Time:\", total_time - datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regression Results\n",
    "\n",
    "We conducted regression on the data set and wanted to determine what variables may affect the departure delay time the most. We conducted the analysis 5 times with different seeds to cross-validate and select different samples. Due to the size of the data set and the limitations of dask, we selected variables based on previous observations and visualizations. The main variables we wanted to observe were the hour the flight was supposed to take off and the distance it was to travel. We conducted this regression twice. The first run was prior to scaling the data to one another while the second took this scaling into account.\n",
    "\n",
    "In our first run, we saw that hour was the dominant factor of these two values in predicting the time of delay for a flight. The hour variable had an average coefficient of 0.727 compared to the 0.001 value of distance. This is a skewed value however because the hour value ranges from 0 to 23 and the distance value can be quite large depending on the flight being conducted. To observe this relationship appropriately, we scale the hour and distance values. After doing this scaling, we see coeffiecients of 3.453 for the departure hour and 0.820 for the distance. This means that the delay time for departure can be calculated with a factor of 3.453 multiplied by the scaled hour and a factor of 0.82 multiplied by the scaled flight distance. The Mean Squared Error was 866.89, slightly larger than the non-scaled analysis of 804.55, with an R Squared value of -0.067.\n",
    "\n",
    "Our second analysis added the age of the plane to the analysis. The age of the plane was represented in number of months and was calculated using the difference between the first recorded flight and the flight being conducted. In this analysis, we continued using the hour of departure and distance of the scheudled flight. When we sclaed the three variables, we saw coefficients of 3.938 for the hour of departure for the flight, 0.623 for the distance of the flight, and 0.154 for the age of the plane. This means that the later in the day the flight is, the longer it is going, and the older the plane is, the longer departure delay the flight will have. The hour plays the largest role in affecting the delay time when we use these coefficients. The Mean Squared Error of this regression for 40% of the data was 866.01, which is just slightly better than the initial regression using just hour of departure and distance of flight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Dask is a new \"big data\" alternative for those preferring the Python language. Although it is in active development by Continuum.io it still lacks certain features, such as, a drop-in generalized linear model. The dask dataframe is easy to use as it follow the popular pandas convention, however, unfortunately it does not allow for row slicing. Row slicing is convient for mini-batch operations that could have been usefull in this case. It is possible to do row slicing with dask arrays, but the data must be on disk using a compatible storage. Parquet is not compatible. Therefore, at this stage of the dask project, dask dataframes work well with out-of-core classification machine learning, and dask arrays are suitable for out-of-core regression for techniques that can utilize mini-batch operations. The dask-glm project looks to tackle out-of-core and distributed regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "* Optimize with index key base on Data, deptarture time, and TailNum\n",
    "* Use of alternative compression, such as snappy or LZ4\n",
    "    * http://java-performance.info/performance-general-compression/\n",
    "* Use a different big data approach to find a more efficient way to estimating the linear model coefficients:\n",
    "    * Spark MLLib\n",
    "    * Turi/Graphlab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* Dask Documentation, http://dask.pydata.org/en/latest/\n",
    "* Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, Boyd, et al http://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf\n",
    "* https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n",
    "* Variable Descriptions: http://stat-computing.org/dataexpo/2009/the-data.html\n",
    "* Dask example using airline data https://jcrist.github.io/dask-sklearn-part-3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A - CSV to Parquet Conversion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert csv to parquet\n",
    "csv_folder = \"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/*.csv\"\n",
    "#parq_folder = \"../data/parquet-tiny/\" \n",
    "#parq_folder = \"../data/parquet_25/\"\n",
    "parq_folder = \"../data/parquet/\"\n",
    "\n",
    "# Variable Metadata\n",
    "var_desc = pd.read_csv(\"../ref/var_descriptions.csv\", index_col='var_id')\n",
    "\n",
    "# Columns for Analysis\n",
    "columns = ['Year', 'Month', 'DayOfWeek', 'Origin','Dest', 'DepTime', 'CRSDepTime',\n",
    "           'DepDelay','ArrDelay', 'UniqueCarrier', 'TailNum', 'Distance']\n",
    "use_vars = var_desc[var_desc['Name'].isin(columns)]\n",
    "\n",
    "\n",
    "def csv_to_parquet(csv_folder, parq_folder, samp_size=1):\n",
    "    start = datetime.now()\n",
    "    df_csv = dd.read_csv(csv_folder,                       \n",
    "                         usecols = use_vars['Name'],\n",
    "                         dtype=dict(use_vars[['Name','Data Type']].values), \n",
    "                         encoding='iso-8859-1')\n",
    "\n",
    "    print(df_csv.head())\n",
    "\n",
    "    # Flip to parquet\n",
    "    df_csv.sample(samp_size).to_parquet(parq_folder,\n",
    "                      compression='gzip',\n",
    "                      object_encoding='utf8')\n",
    "\n",
    "    time_to_complete = datetime.now() - start\n",
    "    print(time_to_complete)\n",
    "\n",
    "csv_to_parquet(csv_folder, parq_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B - Benchmark Tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def benchmark_test(df):\n",
    "    format = lambda x: \"{0:.3f}\".format(x) \n",
    "    start = datetime.now()\n",
    "    print(df[['Distance']].dropna().describe().compute().applymap(format))\n",
    "    time_to_complete = datetime.now() - start\n",
    "    print(time_to_complete)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_par = dd.read_parquet(\"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/parquet/\")\n",
    "%timeit benchmark_test(df_par)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#%timeit Using this magic causes the memory error\n",
    "df_csv = dd.read_csv(\"C:/Users/ryan.shuhart/Downloads/AirlineDelays.tar/AirlineDelays/*.csv\", \n",
    "                 usecols = var_desc['Name'],\n",
    "                 dtype=dict(var_desc[['Name','Data Type']].values), \n",
    "                 encoding='iso-8859-1')\n",
    "\n",
    "start = datetime.now()\n",
    "benchmark_test(df_csv)\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix C - Comparison of Dask Files\n",
    "* Ryan's Hardware: \n",
    "    - CPU: Intel i5-4300M @ 2.60GHz\n",
    "    - Disk: Samsung SSD 850 Pro\n",
    "    - RAM: 8 GB\n",
    "    \n",
    "\n",
    "* Dask using original csv:\n",
    "    - no conversion\n",
    "    - size on disk\n",
    "        - 11.2 gb\n",
    "    - benchmark of describing 'Distance':\n",
    "        - Approx. 4 minutes\n",
    "* Dask using uncompressed parquet: \n",
    "    - conversion to parquet\n",
    "        - approx 10 minutes\n",
    "    - size on disk:\n",
    "        - 13.8 gb\n",
    "    - benchmark of describing 'Distance':\n",
    "        - 1 loop, best of 3: 6.2 s per loop\n",
    "* Dask using gzip compressed parquet:\n",
    "    - converstion to parquet\n",
    "        - approx 42 minutes\n",
    "    - size on disk:\n",
    "        - 1.36 gb <- big difference\n",
    "    - benchmark of describing 'Distance':\n",
    "        - 1 loop, best of 3: 8.83 s per loop\n",
    "\n",
    "#### Summary\n",
    "Dask allows for out of core management of data sets. CSV files are universal, but slow to process. Converting to parquet file format, speeds up the process by a factor of 38. Using the gzip compression, reduces size on disk from 13.8gb to 1.36 or about 10% of the uncompressed size. This comes in handy for a distributed processing in a cluster since not as much network bandwidth would be needed. The trade off of compression is a 42.4% increasing in processing time, however, 3 additional seconds is hardly noticable, but might be more of an issue for other tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix D - Processing, Creation, and Scaling of Features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Location\n",
    "#parq_folder = \"../data/parquet-tiny/\" # Testing\n",
    "#parq_folder = \"../data/parquet_25/\" # Higher Load Testing\n",
    "parq_folder = \"../data/parquet/\" # Full data\n",
    "\n",
    "df = dd.read_parquet(parq_folder)\n",
    "\n",
    "# Create an hour field\n",
    "# 2400 minutes from midnight reduced to 2399 then int division drops to 23\n",
    "df = df.assign(Hour=df.CRSDepTime.astype(float).clip(upper=2399)//100) \n",
    "\n",
    "# Months from 0 AD\n",
    "df['FlightAge'] = 12*df['Year']+df['Month']-1\n",
    "\n",
    "# The months from the first recorded flight is consider the approx age of the plane. \n",
    "# Unfortunately, tail numbers not tracked until 1995. \n",
    "\n",
    "# Find the first year and month of a tail numbers flight history\n",
    "tail_births = (df.groupby('TailNum')[['FlightAge']].min().reset_index()\n",
    "                 .rename(columns={'FlightAge':'FirstFlight'}))\n",
    "\n",
    "df_with_tails = dd.merge(df[df['Year']>1994], tail_births, how='left', on='TailNum')\n",
    "df_with_tails['Age'] = df_with_tails['FlightAge'] - df_with_tails['FirstFlight']\n",
    "\n",
    "#df_with_tails = df_with_tails.drop(['FlightAge','FirstFlight'], axis=1)\n",
    "\n",
    "start = datetime.now()\n",
    "def scaler(df, column):\n",
    "    return (df[column] - df[column].dropna().mean())/df[column].dropna().std()\n",
    "\n",
    "# Scale columns for regression of all data\n",
    "df['Hour_scaled'] = scaler(df, 'Hour')\n",
    "df['Distance_scaled'] = scaler(df, 'Distance')\n",
    "\n",
    "# Scale columns for regression for after 1994\n",
    "df_with_tails['Hour_scaled'] = scaler(df_with_tails, 'Hour')\n",
    "df_with_tails['Distance_scaled'] = scaler(df_with_tails, 'Distance')\n",
    "df_with_tails['Age_scaled'] = scaler(df_with_tails, 'Age')\n",
    "\n",
    "print(\"Time to Build: \", datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "df.to_parquet(parq_folder+\"processed/\", compression='gzip', object_encoding='utf8')\n",
    "print(\"Time to make processed data: \", datetime.now() - start)\n",
    "df_with_tails.to_parquet(parq_folder+\"processed_age/\", compression='gzip', object_encoding='utf8')\n",
    "print(\"Time to make processed data with plane ages: \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
